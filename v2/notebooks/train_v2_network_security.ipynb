{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Security Expert v2 - Fine-Tuning Notebook\n",
    "\n",
    "## Overview\n",
    "This notebook fine-tunes **Llama 3.1 8B Instruct** to create a specialized AI for:\n",
    "- **Conceptual/Theoretical** network security knowledge\n",
    "- **Chain-of-thought reasoning** for complex workflows\n",
    "- **FireWeave orchestration** with function calling\n",
    "- **Compliance framework** expertise (PCI-DSS, SOC2, NIST, HIPAA, ISO 27001)\n",
    "\n",
    "## Key Differences from v1\n",
    "- Focus on WHY (theory) not just WHAT (commands)\n",
    "- Multi-step reasoning chains\n",
    "- Function calling for FireWeave API\n",
    "- Multi-turn conversations\n",
    "- ~20,000 training examples\n",
    "\n",
    "## Requirements\n",
    "- Google Colab with GPU (T4 minimum, A100 recommended)\n",
    "- ~16GB GPU VRAM\n",
    "- Training data: `v2/data/processed/all_training_data_v2.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Unsloth (optimized fine-tuning library)\n",
    "%%capture\n",
    "import torch\n",
    "major_version, minor_version = torch.cuda.get_device_capability()\n",
    "\n",
    "# Install the correct version based on CUDA capability\n",
    "if major_version >= 8:\n",
    "    # Ampere or newer (A100, RTX 30xx, etc.)\n",
    "    !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "else:\n",
    "    # Older GPUs (T4, V100, etc.)\n",
    "    !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "!pip install --no-deps xformers trl peft accelerate bitsandbytes triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Base Model with Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# V2 Configuration - Longer context for reasoning chains\n",
    "max_seq_length = 4096  # Increased for chain-of-thought\n",
    "dtype = None  # Auto-detect\n",
    "load_in_4bit = True  # Use 4-bit quantization for memory efficiency\n",
    "\n",
    "# Load Llama 3.1 8B Instruct (4-bit quantized)\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Max sequence length: {max_seq_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LoRA adapters for efficient fine-tuning\n",
    "# V2: Slightly higher rank for better conceptual understanding\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=32,  # Increased from 16 for better reasoning\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0,  # No dropout for inference stability\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Memory optimization\n",
    "    random_state=42,\n",
    "    use_rslora=True,  # Rank-stabilized LoRA\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your training data or mount Google Drive\n",
    "from google.colab import files, drive\n",
    "\n",
    "# Option 1: Upload directly\n",
    "# uploaded = files.upload()\n",
    "\n",
    "# Option 2: Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set the path to your training data\n",
    "# TRAINING_DATA_PATH = '/content/all_training_data_v2.json'  # If uploaded\n",
    "TRAINING_DATA_PATH = '/content/drive/MyDrive/Finetuning/v2/data/processed/all_training_data_v2.json'  # If from Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load training data\n",
    "with open(TRAINING_DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "    training_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(training_data):,} training examples\")\n",
    "\n",
    "# Analyze data\n",
    "topics = {}\n",
    "has_reasoning = 0\n",
    "has_function = 0\n",
    "multi_turn = 0\n",
    "\n",
    "for item in training_data:\n",
    "    topic = item.get('topic', 'unknown')\n",
    "    topics[topic] = topics.get(topic, 0) + 1\n",
    "    if item.get('has_reasoning', False):\n",
    "        has_reasoning += 1\n",
    "    if item.get('has_function_call', False):\n",
    "        has_function += 1\n",
    "    if len(item.get('conversations', [])) > 2:\n",
    "        multi_turn += 1\n",
    "\n",
    "print(f\"\\nData Distribution:\")\n",
    "print(f\"  Topics: {len(topics)}\")\n",
    "print(f\"  With reasoning: {has_reasoning:,} ({100*has_reasoning/len(training_data):.1f}%)\")\n",
    "print(f\"  With function calls: {has_function:,} ({100*has_function/len(training_data):.1f}%)\")\n",
    "print(f\"  Multi-turn: {multi_turn:,} ({100*multi_turn/len(training_data):.1f}%)\")\n",
    "\n",
    "print(f\"\\nTop 10 Topics:\")\n",
    "for topic, count in sorted(topics.items(), key=lambda x: -x[1])[:10]:\n",
    "    print(f\"  {topic}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V2 System Prompt - Focused on conceptual understanding and orchestration\n",
    "V2_SYSTEM_PROMPT = \"\"\"You are a Network Security Expert AI integrated with FireWeave - an enterprise firewall automation platform.\n",
    "\n",
    "**Your Capabilities:**\n",
    "- Deep understanding of network security CONCEPTS and THEORY\n",
    "- Expertise in compliance frameworks (PCI-DSS, SOC2, NIST, HIPAA, ISO 27001)\n",
    "- Multi-cloud security (AWS, Azure, GCP) and Palo Alto Panorama\n",
    "- Attack path analysis and blast radius calculation\n",
    "- ServiceNow integration for change management\n",
    "\n",
    "**Your Approach:**\n",
    "1. REASON step-by-step through complex problems\n",
    "2. Explain the WHY behind security decisions\n",
    "3. Reference relevant compliance requirements\n",
    "4. Use FireWeave functions when action is needed\n",
    "5. Consider security trade-offs and risks\n",
    "\n",
    "**FireWeave Functions Available:**\n",
    "- check_traffic_flow: Verify if traffic is allowed\n",
    "- analyze_attack_path: Find attack paths and blast radius\n",
    "- run_compliance_scan: Check against compliance frameworks\n",
    "- find_shadowed_rules: Identify policy optimization opportunities\n",
    "- create_firewall_rule: Generate rule configurations\n",
    "- submit_change_request: Create ServiceNow tickets\n",
    "\n",
    "Always prioritize security, explain your reasoning, and provide actionable guidance.\"\"\"\n",
    "\n",
    "# Llama 3 Chat Template for formatting\n",
    "def format_conversation(example):\n",
    "    \"\"\"Format conversation for Llama 3 training.\"\"\"\n",
    "    conversations = example.get('conversations', [])\n",
    "\n",
    "    # Build the formatted text\n",
    "    text = f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{V2_SYSTEM_PROMPT}<|eot_id|>\"\n",
    "\n",
    "    for turn in conversations:\n",
    "        role = turn['from']\n",
    "        content = turn['value']\n",
    "\n",
    "        if role == 'human':\n",
    "            text += f\"<|start_header_id|>user<|end_header_id|>\\n\\n{content}<|eot_id|>\"\n",
    "        elif role == 'gpt':\n",
    "            text += f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n{content}<|eot_id|>\"\n",
    "\n",
    "    return {'text': text}\n",
    "\n",
    "# Convert to dataset\n",
    "dataset = Dataset.from_list(training_data)\n",
    "dataset = dataset.map(format_conversation)\n",
    "\n",
    "print(f\"Dataset prepared: {len(dataset):,} examples\")\n",
    "print(f\"\\nSample formatted text (first 500 chars):\")\n",
    "print(dataset[0]['text'][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "# V2 Training Configuration\n",
    "# Optimized for longer sequences and reasoning\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,  # Disable packing for multi-turn conversations\n",
    "    args=TrainingArguments(\n",
    "        # Output\n",
    "        output_dir=\"./outputs_v2\",\n",
    "\n",
    "        # Training duration\n",
    "        num_train_epochs=3,\n",
    "        max_steps=-1,\n",
    "\n",
    "        # Batch size (adjust based on GPU memory)\n",
    "        per_device_train_batch_size=1,  # Reduced for longer sequences\n",
    "        gradient_accumulation_steps=8,  # Effective batch size = 8\n",
    "\n",
    "        # Learning rate\n",
    "        learning_rate=1e-4,  # Slightly lower for stability\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.05,\n",
    "\n",
    "        # Optimization\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        max_grad_norm=1.0,\n",
    "\n",
    "        # Precision\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "\n",
    "        # Logging\n",
    "        logging_steps=10,\n",
    "        logging_dir=\"./logs_v2\",\n",
    "\n",
    "        # Checkpointing\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=500,\n",
    "        save_total_limit=3,\n",
    "\n",
    "        # Other\n",
    "        seed=42,\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Trainer configured!\")\n",
    "print(f\"  Epochs: 3\")\n",
    "print(f\"  Effective batch size: 8\")\n",
    "print(f\"  Learning rate: 1e-4\")\n",
    "print(f\"  Max sequence length: {max_seq_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting v2 training...\")\n",
    "print(f\"Training on {len(dataset):,} examples\")\n",
    "print(f\"This will take several hours depending on your GPU.\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"Training complete!\")\n",
    "print(f\"  Training time: {trainer_stats.metrics['train_runtime']:.0f} seconds\")\n",
    "print(f\"  Final loss: {trainer_stats.metrics['train_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "def ask_v2_model(question: str, max_new_tokens: int = 1024) -> str:\n",
    "    \"\"\"Ask the v2 model a question.\"\"\"\n",
    "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{V2_SYSTEM_PROMPT}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    # Extract assistant response\n",
    "    assistant_start = response.rfind(\"<|start_header_id|>assistant<|end_header_id|>\")\n",
    "    if assistant_start != -1:\n",
    "        response = response[assistant_start + len(\"<|start_header_id|>assistant<|end_header_id|>\"):]\n",
    "        response = response.replace(\"<|eot_id|>\", \"\").strip()\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Chain-of-thought reasoning\n",
    "print(\"Test 1: Chain-of-thought reasoning\")\n",
    "print(\"=\"*50)\n",
    "response = ask_v2_model(\n",
    "    \"We need to allow our web servers to access a new payment API. Walk me through the security considerations and what compliance requirements apply.\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Function calling\n",
    "print(\"\\nTest 2: Function calling\")\n",
    "print(\"=\"*50)\n",
    "response = ask_v2_model(\n",
    "    \"Check if traffic from 10.1.1.100 to our database server 192.168.50.10 on port 5432 is allowed, and tell me the security implications.\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Compliance knowledge\n",
    "print(\"\\nTest 3: Compliance knowledge\")\n",
    "print(\"=\"*50)\n",
    "response = ask_v2_model(\n",
    "    \"What are the key PCI-DSS requirements for firewall configuration, and how can FireWeave help us achieve compliance?\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Attack path analysis\n",
    "print(\"\\nTest 4: Attack path analysis\")\n",
    "print(\"=\"*50)\n",
    "response = ask_v2_model(\n",
    "    \"Explain how to analyze attack paths from the internet to our internal database tier, and what factors determine blast radius.\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapter\n",
    "model.save_pretrained(\"/content/network-security-expert-v2-lora\")\n",
    "tokenizer.save_pretrained(\"/content/network-security-expert-v2-lora\")\n",
    "print(\"LoRA adapter saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge and save as 16-bit\n",
    "print(\"Merging LoRA with base model...\")\n",
    "model.save_pretrained_merged(\n",
    "    \"/content/network-security-expert-v2-merged\",\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\"\n",
    ")\n",
    "print(\"Merged model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export to GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to GGUF for Ollama deployment\n",
    "print(\"Converting to GGUF format...\")\n",
    "print(\"This will create Q4_K_M, Q5_K_M, and Q8_0 versions.\")\n",
    "\n",
    "model.save_pretrained_gguf(\n",
    "    \"/content/network-security-expert-v2-gguf\",\n",
    "    tokenizer,\n",
    "    quantization_method=[\"q4_k_m\", \"q5_k_m\", \"q8_0\"]\n",
    ")\n",
    "\n",
    "print(\"\\nGGUF files created:\")\n",
    "!ls -lh /content/network-security-expert-v2-gguf/*.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy to Google Drive for download\n",
    "!mkdir -p /content/drive/MyDrive/Finetuning/v2/models/gguf\n",
    "!cp /content/network-security-expert-v2-gguf/*.gguf /content/drive/MyDrive/Finetuning/v2/models/gguf/\n",
    "print(\"GGUF files copied to Google Drive!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Create Modelfile for Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Modelfile for v2\n",
    "v2_modelfile = f'''# Ollama Modelfile for Network Security Expert v2\n",
    "#\n",
    "# This version focuses on:\n",
    "# - Conceptual/theoretical understanding\n",
    "# - Chain-of-thought reasoning\n",
    "# - FireWeave orchestration\n",
    "# - Compliance framework expertise\n",
    "\n",
    "FROM ./Llama-3.1-8B-Instruct.Q4_K_M.gguf\n",
    "\n",
    "# Llama 3 Chat Template\n",
    "TEMPLATE \"\"\"{{{{ if .System }}}}<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{{{{ .System }}}}<|eot_id|>{{{{ end }}}}{{{{ if .Prompt }}}}<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{{{{ .Prompt }}}}<|eot_id|>{{{{ end }}}}<|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{{{{ .Response }}}}<|eot_id|>\"\"\"\n",
    "\n",
    "# Stop tokens\n",
    "PARAMETER stop \"<|start_header_id|>\"\n",
    "PARAMETER stop \"<|end_header_id|>\"\n",
    "PARAMETER stop \"<|eot_id|>\"\n",
    "PARAMETER stop \"<|reserved_special_token\"\n",
    "\n",
    "# Generation parameters\n",
    "PARAMETER temperature 0.7\n",
    "PARAMETER top_p 0.9\n",
    "PARAMETER top_k 40\n",
    "PARAMETER repeat_penalty 1.1\n",
    "PARAMETER num_ctx 4096\n",
    "\n",
    "# V2 System Prompt\n",
    "SYSTEM \"\"\"{V2_SYSTEM_PROMPT}\"\"\"\n",
    "'''\n",
    "\n",
    "# Save Modelfile\n",
    "with open('/content/network-security-expert-v2-gguf/Modelfile', 'w') as f:\n",
    "    f.write(v2_modelfile)\n",
    "\n",
    "# Copy to Drive\n",
    "!cp /content/network-security-expert-v2-gguf/Modelfile /content/drive/MyDrive/Finetuning/v2/models/\n",
    "\n",
    "print(\"Modelfile created!\")\n",
    "print(\"\\nTo import to Ollama:\")\n",
    "print(\"  cd /path/to/gguf/folder\")\n",
    "print(\"  ollama create network-security-expert-v2 -f Modelfile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Upload to Hugging Face (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Upload to Hugging Face Hub\n",
    "# Uncomment and run if you want to share the model\n",
    "\n",
    "# !pip install huggingface_hub\n",
    "# from huggingface_hub import login, HfApi, create_repo\n",
    "\n",
    "# login()  # Enter your HF token\n",
    "\n",
    "# HF_USERNAME = \"YOUR_USERNAME\"  # Change this!\n",
    "# REPO_NAME = \"network-security-expert-v2-gguf\"\n",
    "\n",
    "# repo_id = f\"{HF_USERNAME}/{REPO_NAME}\"\n",
    "# create_repo(repo_id, repo_type=\"model\", exist_ok=True)\n",
    "\n",
    "# api = HfApi()\n",
    "# api.upload_folder(\n",
    "#     folder_path=\"/content/network-security-expert-v2-gguf\",\n",
    "#     repo_id=repo_id,\n",
    "#     repo_type=\"model\",\n",
    "#     commit_message=\"Upload Network Security Expert v2 GGUF\"\n",
    "# )\n",
    "\n",
    "# print(f\"Uploaded to: https://huggingface.co/{repo_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**v2 Model Training Complete!**\n",
    "\n",
    "Key differences from v1:\n",
    "- LoRA rank: 32 (vs 16) for better conceptual understanding\n",
    "- Context length: 4096 (vs 2048) for reasoning chains\n",
    "- Learning rate: 1e-4 (vs 2e-4) for stability\n",
    "- Training data: ~20,000 conceptual examples with chain-of-thought\n",
    "\n",
    "**Next Steps:**\n",
    "1. Download GGUF file from Google Drive\n",
    "2. Place in `v2/models/gguf/`\n",
    "3. Run: `ollama create network-security-expert-v2 -f Modelfile`\n",
    "4. Test: `ollama run network-security-expert-v2`\n",
    "\n",
    "**Test Questions:**\n",
    "- \"Explain the principle of least privilege and how it applies to firewall rules\"\n",
    "- \"What are the PCI-DSS requirements for network segmentation?\"\n",
    "- \"Analyze the attack path from our DMZ to the database tier\"\n",
    "- \"Check if traffic from 10.0.0.1 to 192.168.1.50 on port 443 is allowed\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
