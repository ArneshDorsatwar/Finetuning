{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Fine-tuning Llama 3.1 8B for Network Security Expert v2\n\nThis notebook fine-tunes Llama 3.1 8B Instruct to create a specialized **Network Security Expert AI** with:\n- **Advanced tool calling** using native Llama 3.1 format (`<|python_tag|>`)\n- **FireWeave orchestration** capabilities\n- **Infosec conversational expertise**\n\n**Training Configuration (Local VM - RTX 3090/4090 Optimized):**\n- LoRA: r=32, alpha=32, dropout=0, **rsLoRA enabled**\n- Learning rate: 2e-4 with cosine scheduler\n- Max sequence length: 2048 (covers 99% of training data)\n- **Packing disabled** for stability\n- **NEFTune noise** (alpha=5) for better generalization\n- Batch size: 2 with gradient accumulation 4 (effective=8)\n\n**Runtime:** Local Ubuntu VM with GPU passthrough (RTX 3090/4090)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "Run this cell if packages aren't installed yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Environment setup for local VM (packages already installed via pip)\nimport os\n\n# Prevent Triton timeout issues\nos.environ[\"TRITON_CACHE_MANAGER\"] = \"unsloth.triton_cache:TritonCacheManager\"\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\"\n\n# Clear Triton cache if it exists\nimport shutil\nfrom pathlib import Path\ntriton_cache = Path.home() / \".triton\" / \"cache\"\nif triton_cache.exists():\n    print(f\"Clearing stale Triton cache...\")\n    shutil.rmtree(triton_cache, ignore_errors=True)\n\nprint(\"âœ“ Environment configured for stable training\")\nprint(\"âœ“ Running on local VM (packages pre-installed)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model\n",
    "\n",
    "Load Llama 3.1 8B Instruct with 4-bit quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from unsloth import FastLanguageModel\nimport torch\nimport os\n\n# Fix for Triton timeout issues on some systems\nos.environ[\"TRITON_CACHE_MANAGER\"] = \"unsloth.triton_cache:TritonCacheManager\"\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\"  # Set to \"1\" only for debugging\n\n# Configuration (Optimized for stability)\nmax_seq_length = 2048  # Reduced from 8192 - covers 99% of training data\ndtype = None  # Auto-detect (bfloat16 for Ampere+)\nload_in_4bit = True  # Use 4-bit quantization for QLoRA\n\n# Load model\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/llama-3.1-8b-Instruct-bnb-4bit\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n)\n\nprint(f\"âœ“ Model loaded: Llama 3.1 8B Instruct (4-bit)\")\nprint(f\"âœ“ Max sequence length: {max_seq_length}\")\nprint(f\"âœ“ Data type: {dtype if dtype else 'Auto (bf16 on Ampere+)'}\")\n\n# GPU memory info\nif torch.cuda.is_available():\n    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3\n    gpu_used = torch.cuda.memory_allocated(0) / 1024**3\n    print(f\"âœ“ GPU: {torch.cuda.get_device_name(0)} ({gpu_mem:.1f} GB total, {gpu_used:.1f} GB used)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Configure LoRA (2025 Best Practices)\n\nAdd LoRA adapters with **rsLoRA** (rank-stabilized LoRA) for better scaling at higher ranks."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Add LoRA adapters (2025 Best Practices with rsLoRA)\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 32,  # Higher rank with rsLoRA (was 16)\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha = 32,  # Match rank when using rsLoRA\n    lora_dropout = 0,  # Unsloth recommends 0 dropout\n    bias = \"none\",\n    use_gradient_checkpointing = \"unsloth\",  # 30% less VRAM\n    random_state = 3407,\n    use_rslora = True,  # NEW: Rank-stabilized LoRA for better high-rank performance\n    loftq_config = None,\n)\n\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\n\nprint(\"âœ“ LoRA adapters configured (2025 Best Practices)\")\nprint(f\"  - Rank: 32 (increased from 16)\")\nprint(f\"  - Alpha: 32 (matched with rank for rsLoRA)\")\nprint(f\"  - Dropout: 0 (Unsloth recommended)\")\nprint(f\"  - rsLoRA: Enabled (scales by 1/sqrt(r) instead of 1/r)\")\nprint(f\"  - Target modules: All attention and MLP layers\")\nprint(f\"  - Trainable parameters: ~{trainable_params / 1e6:.1f}M ({100 * trainable_params / total_params:.2f}%)\")\nprint(f\"\\n[rsLoRA] Better performance at higher ranks with proper gradient scaling\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Dataset\n",
    "\n",
    "Load your network security training data in ChatML/ShareGPT format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from datasets import load_dataset\nimport json\nimport os\n\n# Training-ready dataset (pre-formatted with Llama 3.1 chat template)\n# Generated by: python scripts/prepare_for_training.py\ndataset_path = os.path.expanduser(\"~/finetuning/data/processed/combined_train_formatted.json\")\n\n# Alternative paths to try\nalt_paths = [\n    \"data/processed/combined_train_formatted.json\",\n    \"../data/processed/combined_train_formatted.json\",\n]\n\n# Find the dataset\nif not os.path.exists(dataset_path):\n    for alt in alt_paths:\n        if os.path.exists(alt):\n            dataset_path = alt\n            break\n\ntry:\n    dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n    print(f\"Dataset loaded from: {dataset_path}\")\n    print(f\"Total examples: {len(dataset)}\")\n    \n    # Count tool calling vs conversational\n    tool_count = sum(1 for ex in dataset if '<|python_tag|>' in str(ex.get('text', '')))\n    print(f\"  Tool calling: {tool_count} ({100*tool_count/len(dataset):.1f}%)\")\n    print(f\"  Knowledge/conversational: {len(dataset) - tool_count}\")\n    \n    # Show format sample\n    print(\"\\nSample (first 400 chars):\")\n    print(\"-\" * 80)\n    print(dataset[0]['text'][:400])\n    print(\"...\")\n    \nexcept FileNotFoundError:\n    print(f\"Dataset not found!\")\n    print(f\"\\nSearched locations:\")\n    print(f\"  - {dataset_path}\")\n    for alt in alt_paths:\n        print(f\"  - {alt}\")\n    print(\"\\nGenerate it with: python scripts/prepare_for_training.py\")\n    raise"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Verify Dataset Format\n\nThe dataset is pre-formatted with the Llama 3.1 chat template (by `scripts/prepare_for_training.py`).\nEach example has a `text` field with the complete formatted conversation including special tokens.\n\nThis cell verifies the format is correct and sets up the tokenizer."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from unsloth.chat_templates import get_chat_template\n\n# Apply Llama 3.1 chat template to tokenizer\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"llama-3.1\",\n)\n\n# Verify special tokens are properly configured\nspecial_tokens = {\n    \"<|begin_of_text|>\": 128000,\n    \"<|end_of_text|>\": 128001,\n    \"<|start_header_id|>\": 128006,\n    \"<|end_header_id|>\": 128007,\n    \"<|eot_id|>\": 128009,\n    \"<|python_tag|>\": 128010,\n}\n\nprint(\"Special token verification:\")\nall_ok = True\nfor token_str, expected_id in special_tokens.items():\n    actual_id = tokenizer.convert_tokens_to_ids(token_str)\n    status = \"OK\" if actual_id == expected_id else \"MISMATCH\"\n    if status != \"OK\":\n        all_ok = False\n    print(f\"  {token_str}: {actual_id} (expected {expected_id}) [{status}]\")\n\nif all_ok:\n    print(\"\\nAll special tokens verified!\")\nelse:\n    print(\"\\nWARNING: Token ID mismatches detected!\")\n\n# Data is already pre-formatted â€” just verify a sample\nprint(\"\\nDataset format: pre-formatted Llama 3.1 text (no additional formatting needed)\")\nprint(f\"Text field present: {'text' in dataset.column_names}\")\n\n# Quick validation\nsample = dataset[0]['text']\nchecks = {\n    \"Starts with <|begin_of_text|>\": sample.startswith(\"<|begin_of_text|>\"),\n    \"Has system header\": \"<|start_header_id|>system<|end_header_id|>\" in sample,\n    \"Has user header\": \"<|start_header_id|>user<|end_header_id|>\" in sample,\n    \"Has assistant header\": \"<|start_header_id|>assistant<|end_header_id|>\" in sample,\n    \"Ends with <|end_of_text|>\": sample.endswith(\"<|end_of_text|>\"),\n    \"No <|eom_id|> (wrong token)\": \"<|eom_id|>\" not in sample,\n}\nprint(\"\\nFormat checks:\")\nfor check, passed in checks.items():\n    print(f\"  [{'PASS' if passed else 'FAIL'}] {check}\")\n\nprint(f\"\\nReady for training with {len(dataset)} examples\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Configure Training (Stability Optimized)\n\nSet up training hyperparameters optimized for stability:\n- **Batch size 1**: Prevents Triton kernel timeout\n- **Gradient accumulation 8**: Maintains effective batch size of 8\n- **No packing**: Avoids creating very long packed sequences"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from trl import SFTTrainer\nfrom transformers import TrainingArguments\nimport os\n\n# Create output directory\nos.makedirs(\"outputs/network-security-v2\", exist_ok=True)\n\n# Training configuration (Optimized for RTX 3090/4090 with 24GB VRAM)\ntraining_args = TrainingArguments(\n    # Output\n    output_dir = \"outputs/network-security-v2\",\n    \n    # Batch size - increased for 24GB VRAM\n    per_device_train_batch_size = 2,  # Can use 2 with 24GB VRAM\n    gradient_accumulation_steps = 4,  # Effective batch size = 8\n    \n    # Training duration\n    num_train_epochs = 3,\n    \n    # Learning rate (standard for QLoRA)\n    learning_rate = 2e-4,\n    lr_scheduler_type = \"cosine\",\n    warmup_ratio = 0.03,\n    \n    # Optimization\n    weight_decay = 0.01,\n    max_grad_norm = 0.3,  # Gradient clipping for stability\n    optim = \"adamw_8bit\",\n    \n    # Logging & saving\n    logging_steps = 10,\n    save_strategy = \"steps\",\n    save_steps = 500,\n    save_total_limit = 2,\n    \n    # Mixed precision\n    fp16 = not torch.cuda.is_bf16_supported(),\n    bf16 = torch.cuda.is_bf16_supported(),\n    \n    # Misc\n    seed = 3407,\n    report_to = \"none\",\n    \n    # Local VM settings\n    dataloader_num_workers = 2,  # Can use more workers locally\n)\n\nprint(\"âœ“ Training configuration (RTX 3090/4090 Optimized):\")\nprint(f\"  - Epochs: {training_args.num_train_epochs}\")\nprint(f\"  - Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"  - Gradient accumulation: {training_args.gradient_accumulation_steps}\")\nprint(f\"  - Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"  - Learning rate: {training_args.learning_rate}\")\nprint(f\"  - LR Scheduler: {training_args.lr_scheduler_type}\")\nprint(f\"  - Warmup ratio: {training_args.warmup_ratio}\")\nprint(f\"  - Gradient clipping: {training_args.max_grad_norm}\")\nprint(f\"  - Mixed precision: {'BF16' if training_args.bf16 else 'FP16'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Initialize Trainer (Stability Settings)\n\nCreate the SFTTrainer with stability optimizations:\n- **Packing disabled**: Prevents creating extremely long sequences that timeout\n- **NEFTune**: Noisy embeddings for better generalization\n- **Single process**: Avoids multiprocessing issues in local runtime"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from unsloth import is_bfloat16_supported\n\n# Stability settings (packing disabled to prevent Triton timeout)\nUSE_PACKING = False     # Disabled - can cause very long sequences and timeout\nNEFTUNE_ALPHA = 5.0     # Noisy embeddings for generalization\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 1,  # Single process for stability\n    packing = USE_PACKING,\n    neftune_noise_alpha = NEFTUNE_ALPHA,\n    args = training_args,\n)\n\nprint(\"âœ“ Trainer initialized (Stability Optimized)\")\nprint(f\"\\n[Configuration]\")\nprint(f\"  Packing: {USE_PACKING} (disabled for stability)\")\nprint(f\"  NEFTune: alpha={NEFTUNE_ALPHA}\")\nprint(f\"  Max seq length: {max_seq_length}\")\nprint(f\"\\nTraining {len(dataset)} examples\")\nprint(f\"Estimated steps per epoch: ~{len(dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)}\")\nprint(f\"\\nNote: If training still times out, try reducing max_seq_length to 1024\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Start Training\n\nBegin the fine-tuning process. This cell first clears the Triton cache to prevent stale kernel issues.\n\n**Target loss:** 0.5-1.0 is generally good\n**Red flags:**\n- Loss not decreasing â†’ adjust learning rate\n- Loss near 0 â†’ overfitting, reduce epochs\n- \"Triton Error: launch timed out\" â†’ reduce batch size or sequence length\n\n**Common warnings (can be ignored):**\n- \"Model is already on multiple devices\" - normal for QLoRA\n- NEFTune warnings - normal during training"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clear any stale Triton cache before training\nimport shutil\nfrom pathlib import Path\n\ntriton_cache = Path.home() / \".triton\" / \"cache\"\nif triton_cache.exists():\n    print(f\"Clearing Triton cache at {triton_cache}...\")\n    shutil.rmtree(triton_cache, ignore_errors=True)\n    print(\"âœ“ Triton cache cleared\")\n\n# Note about \"model is already on multiple devices\" warning\nprint(\"\\nNote: 'Model is already on multiple devices' warning is normal and can be ignored.\")\nprint(\"=\"*50)\n\n# Start training\nprint(\"\\nStarting v2 training...\")\nprint(f\"Training on {len(dataset)} examples\")\nprint(f\"This will take several hours depending on your GPU.\")\nprint(\"=\"*50)\n\ntrainer_stats = trainer.train()\n\nprint(\"=\"*50)\nprint(\"Training complete!\")\nprint(f\"Final loss: {trainer_stats.training_loss:.4f}\")\nprint(f\"Training time: {trainer_stats.metrics['train_runtime']/3600:.2f} hours\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test the Model\n",
    "\n",
    "Try out your fine-tuned model with some network security questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Enable inference mode\nFastLanguageModel.for_inference(model)\n\nimport json\n\n# Test 1: Tool calling (should produce <|python_tag|>)\nprint(\"=\" * 80)\nprint(\"TEST 1: Tool Calling (should produce <|python_tag|> with correct JSON)\")\nprint(\"=\" * 80)\n\ntool_test_prompt = (\n    \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n    \"You are Ember, a senior network security analyst embedded in the FireWeave platform.\\n\\n\"\n    \"RULES:\\n\"\n    \"- NEVER fabricate data. Only present data from tool results.\\n\"\n    \"- Don't narrate your process. Present results directly.\\n\\n\"\n    \"Environment: ipython\\n\\n\"\n    '{\"name\": \"search_objects\", \"description\": \"Search for address and service objects\", '\n    '\"parameters\": {\"type\": \"object\", \"properties\": {\"query\": {\"type\": \"string\"}}, \"required\": [\"query\"]}}'\n    \"<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n    \"find 10.0.0.1<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n)\n\ninputs = tokenizer(tool_test_prompt, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**inputs, max_new_tokens=100, temperature=0.1)\ngenerated = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:])\nprint(f\"Generated: {generated}\")\n\nif \"<|python_tag|>\" in generated:\n    json_part = generated.split(\"<|python_tag|>\")[1].split(\"<|eot_id|>\")[0].strip()\n    try:\n        tc = json.loads(json_part)\n        has_name = \"name\" in tc\n        has_params = \"parameters\" in tc\n        no_openai = \"function\" not in tc\n        print(f\"\\nTool call JSON: {json.dumps(tc, indent=2)}\")\n        print(f\"[{'PASS' if has_name else 'FAIL'}] Has 'name' field\")\n        print(f\"[{'PASS' if has_params else 'FAIL'}] Has 'parameters' field\")\n        print(f\"[{'PASS' if no_openai else 'FAIL'}] Not OpenAI format\")\n    except json.JSONDecodeError:\n        print(f\"[FAIL] Invalid JSON: {json_part}\")\nelse:\n    print(\"[FAIL] No <|python_tag|> in output\")\n\n# Test 2: Knowledge question (should NOT produce tool call)\nprint(\"\\n\" + \"=\" * 80)\nprint(\"TEST 2: Knowledge Question (should answer from knowledge, NO tool call)\")\nprint(\"=\" * 80)\n\nknowledge_prompt = (\n    \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n    \"You are Ember, a senior network security analyst embedded in the FireWeave platform.\\n\\n\"\n    \"Environment: ipython\\n\\n\"\n    '{\"name\": \"search_objects\", \"description\": \"Search objects\", '\n    '\"parameters\": {\"type\": \"object\", \"properties\": {\"query\": {\"type\": \"string\"}}, \"required\": [\"query\"]}}'\n    \"<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n    \"what is a shadowed rule?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n)\n\ninputs = tokenizer(knowledge_prompt, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**inputs, max_new_tokens=256, temperature=0.7, top_p=0.9)\ngenerated = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\nprint(f\"Generated:\\n{generated[:500]}\")\nhas_tool = \"<|python_tag|>\" in tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:])\nprint(f\"\\n[{'PASS' if not has_tool else 'FAIL'}] No tool call for knowledge question\")\n\n# Test 3: General security question\nprint(\"\\n\" + \"=\" * 80)\nprint(\"TEST 3: General Security Question\")\nprint(\"=\" * 80)\n\nmessages = [{\"role\": \"user\", \"content\": \"How do I configure port security on a Cisco switch?\"}]\ninputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(input_ids=inputs, max_new_tokens=512, temperature=0.7, top_p=0.9, do_sample=True)\nresponse = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\nresponse = response.split(\"assistant\\n\\n\")[-1] if \"assistant\" in response else response\nprint(f\"Answer:\\n{response[:500]}\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Testing complete!\")\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save the Model\n",
    "\n",
    "Save the LoRA adapter (small ~100-200MB file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\n# Create models directory\nos.makedirs(\"models/network-security-lora\", exist_ok=True)\n\n# Save LoRA adapter locally\nmodel.save_pretrained(\"models/network-security-lora\")\ntokenizer.save_pretrained(\"models/network-security-lora\")\n\nprint(\"âœ“ LoRA adapter saved to: models/network-security-lora\")\nprint(f\"  Size: ~100-200 MB\")\nprint(\"\\nTo upload to Hugging Face Hub, uncomment the cell below.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Push to Hugging Face Hub\n",
    "# Replace 'your-username' with your HF username\n",
    "\n",
    "# model.push_to_hub(\"your-username/llama3-network-security-lora\", token=\"YOUR_HF_TOKEN\")\n",
    "# tokenizer.push_to_hub(\"your-username/llama3-network-security-lora\", token=\"YOUR_HF_TOKEN\")\n",
    "\n",
    "# print(\"âœ“ Model uploaded to Hugging Face Hub!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export to GGUF for Ollama\n",
    "\n",
    "Convert to GGUF format for use with Ollama on your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\n# Create output directories\nos.makedirs(\"models/merged-16bit\", exist_ok=True)\nos.makedirs(\"models/gguf\", exist_ok=True)\n\n# First, save merged 16-bit model\nprint(\"Step 1: Merging LoRA with base model...\")\nprint(\"(This may take a few minutes...)\")\nmodel.save_pretrained_merged(\n    \"models/merged-16bit\",\n    tokenizer,\n    save_method=\"merged_16bit\"\n)\nprint(\"âœ“ Merged model saved\\n\")\n\n# Convert to GGUF with multiple quantization levels\nprint(\"Step 2: Converting to GGUF format...\")\nprint(\"This will create 3 quantized versions (Q4_K_M, Q5_K_M, Q8_0)\")\nprint(\"(This takes 10-30 minutes depending on your CPU...)\\n\")\n\nmodel.save_pretrained_gguf(\n    \"models/gguf\",\n    tokenizer,\n    quantization_method=[\"q4_k_m\", \"q5_k_m\", \"q8_0\"]\n)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"âœ… GGUF CONVERSION COMPLETE!\")\nprint(\"=\"*80)\nprint(\"\\nCreated files in models/gguf/:\")\nprint(\"  - unsloth.Q4_K_M.gguf (~4.5GB) - Fastest, good quality\")\nprint(\"  - unsloth.Q5_K_M.gguf (~5.5GB) - Balanced [RECOMMENDED]\")\nprint(\"  - unsloth.Q8_0.gguf (~8GB) - Highest quality\")\nprint(\"\\n\" + \"=\"*80)\nprint(\"NEXT STEPS (run in terminal):\")\nprint(\"=\"*80)\nprint(\"\"\"\n# 1. Rename the GGUF file\nmv models/gguf/unsloth.Q4_K_M.gguf models/gguf/network-security-expert.Q4_K_M.gguf\n\n# 2. Create Ollama model\ncd models\nollama create network-security-expert -f Modelfile\n\n# 3. Test your model\nollama run network-security-expert\n\"\"\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nYou've fine-tuned Llama 3.1 8B to be a Network Security expert on your local VM!\n\n### What You've Done:\n1. âœ… Loaded Llama 3.1 8B Instruct with 4-bit quantization\n2. âœ… Configured LoRA adapters for efficient training\n3. âœ… Trained on your network security dataset\n4. âœ… Tested the model with example questions\n5. âœ… Saved the LoRA adapter\n6. âœ… Converted to GGUF for Ollama deployment\n\n### Files Created:\n- `models/network-security-lora/` - LoRA adapter (~100-200MB)\n- `models/merged-16bit/` - Full merged model\n- `models/gguf/*.gguf` - Quantized models for Ollama\n\n### Deploy with Ollama:\n```bash\n# Install Ollama (if not already)\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Create and run your model\ncd ~/finetuning/models\nollama create network-security-expert -f Modelfile\nollama run network-security-expert\n```\n\n### Test Your Model:\n```\n>>> How do I configure a Palo Alto firewall rule?\n>>> What's the difference between IDS and IPS?\n>>> Explain zero trust architecture\n```\n\nðŸŽ‰ Training complete!"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}