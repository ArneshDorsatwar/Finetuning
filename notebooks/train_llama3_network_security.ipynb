{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Fine-tuning Llama 3.1 8B for Network Security Expert v2\n\nThis notebook fine-tunes Llama 3.1 8B Instruct to create a specialized **Network Security Expert AI** with:\n- **Advanced tool calling** using native Llama 3.1 format (`<|python_tag|>`)\n- **FireWeave orchestration** capabilities\n- **Infosec conversational expertise**\n\n**Training Configuration (Optimized for Tool Calling):**\n- LoRA: r=16, alpha=32, dropout=0.05\n- Learning rate: 3e-4 with cosine scheduler\n- Max sequence length: 4096 for multi-turn conversations\n- Dataset: 17,341 examples (41% tool calling, 59% conversational)\n\n**Runtime:** Use GPU (T4/A100) - Runtime > Change runtime type > GPU"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "Run this cell if packages aren't installed yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Unsloth and dependencies\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model\n",
    "\n",
    "Load Llama 3.1 8B Instruct with 4-bit quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from unsloth import FastLanguageModel\nimport torch\n\n# Configuration (Optimized for Tool Calling)\nmax_seq_length = 4096  # Increased for multi-turn tool calling conversations\ndtype = None  # Auto-detect (float16 for T4, bfloat16 for Ampere+)\nload_in_4bit = True  # Use 4-bit quantization for QLoRA\n\n# Load model\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/llama-3.1-8b-Instruct-bnb-4bit\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n)\n\nprint(f\"‚úì Model loaded: Llama 3.1 8B Instruct (4-bit)\")\nprint(f\"‚úì Max sequence length: {max_seq_length}\")\nprint(f\"‚úì Data type: {dtype if dtype else 'Auto'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure LoRA\n",
    "\n",
    "Add LoRA adapters for parameter-efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Add LoRA adapters (Optimized for Tool Calling)\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16,  # LoRA rank\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha = 32,  # 2x rank for better learning\n    lora_dropout = 0.05,  # Regularization for tool calling\n    bias = \"none\",\n    use_gradient_checkpointing = \"unsloth\",  # Memory efficient\n    random_state = 42,\n    use_rslora = False,\n    loftq_config = None,\n)\n\nprint(\"‚úì LoRA adapters configured (Optimized for Tool Calling)\")\nprint(f\"  - Rank: 16\")\nprint(f\"  - Alpha: 32 (2x rank)\")\nprint(f\"  - Dropout: 0.05\")\nprint(f\"  - Target modules: All attention and MLP layers\")\nprint(f\"  - Trainable parameters: ~{sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.1f}M\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Dataset\n",
    "\n",
    "Load your network security training data in ChatML/ShareGPT format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from datasets import load_dataset\nimport json\n\n# Upload your training data to Colab first:\n# 1. Click the folder icon on the left\n# 2. Upload: v2/data/processed/training_data_final.json (76.9 MB)\n\ndataset_path = \"training_data_final.json\"\n\ntry:\n    dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n    print(f\"‚úì Dataset loaded: {len(dataset)} examples\")\n    \n    # Count tool calling vs conversational\n    tool_count = sum(1 for ex in dataset if '<|python_tag|>' in str(ex.get('conversations', [])))\n    print(f\"  - Tool calling: {tool_count} ({100*tool_count/len(dataset):.1f}%)\")\n    print(f\"  - Conversational: {len(dataset) - tool_count}\")\n    \n    # Show a sample\n    print(\"\\nSample conversation:\")\n    print(\"-\" * 80)\n    sample = dataset[0]['conversations']\n    for msg in sample[:2]:\n        role = msg['from']\n        text = msg['value'][:200]\n        print(f\"{role.upper()}: {text}...\\n\")\n    \nexcept FileNotFoundError:\n    print(f\"‚ùå Dataset not found at {dataset_path}\")\n    print(\"\\nPlease upload your training data:\")\n    print(\"1. Click folder icon on left panel\")\n    print(\"2. Upload: v2/data/processed/training_data_final.json\")\n    raise"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Format Dataset for Training\n",
    "\n",
    "Apply Llama 3 chat template to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from unsloth.chat_templates import get_chat_template\nimport json\n\n# Apply Llama 3.1 chat template\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"llama-3.1\",\n)\n\n# System prompt for tool calling\nSYSTEM_PROMPT = \"\"\"You are a Network Security Expert AI with FireWeave orchestration capabilities.\n\nAvailable tools: check_traffic_flow, analyze_attack_path, run_compliance_scan, find_shadowed_rules, create_firewall_rule, get_rule_hit_count, calculate_blast_radius, fetch_jira_issues\n\nWhen calling tools, use the format: <|python_tag|>{\"name\": \"tool_name\", \"parameters\": {...}}\n\nProvide accurate, detailed technical guidance with specific commands and configurations.\"\"\"\n\ndef formatting_prompts_func(examples):\n    \"\"\"Format conversations for Llama 3.1 native tool calling.\"\"\"\n    conversations = examples[\"conversations\"]\n    tools_list = examples.get(\"tools\", [None] * len(conversations))\n    texts = []\n    \n    for convo, tools in zip(conversations, tools_list):\n        # Build text manually for proper tool calling format\n        text = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n        text += SYSTEM_PROMPT\n        \n        # Add tool definitions if available\n        if tools:\n            text += \"\\n\\nAvailable tools:\\n\"\n            text += json.dumps(tools, indent=2)\n        \n        text += \"<|eot_id|>\"\n        \n        for turn in convo:\n            role = turn.get(\"from\", \"\")\n            value = turn.get(\"value\", \"\")\n            \n            if role == \"human\":\n                text += f\"<|start_header_id|>user<|end_header_id|>\\n\\n{value}<|eot_id|>\"\n            elif role == \"gpt\":\n                # Check if this is a tool call (contains <|python_tag|>)\n                if \"<|python_tag|>\" in value:\n                    # Tool call ends with <|eom_id|> (end of message, expecting tool response)\n                    text += f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n{value}<|eom_id|>\"\n                else:\n                    # Regular response ends with <|eot_id|>\n                    text += f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n{value}<|eot_id|>\"\n            elif role == \"tool\":\n                # Tool response uses ipython role\n                text += f\"<|start_header_id|>ipython<|end_header_id|>\\n\\n{value}<|eot_id|>\"\n        \n        texts.append(text)\n    \n    return {\"text\": texts}\n\n# Apply formatting\ndataset = dataset.map(formatting_prompts_func, batched=True)\n\nprint(\"‚úì Native Llama 3.1 tool calling format applied\")\nprint(\"\\nFormatted example (first 800 chars):\")\nprint(\"-\" * 80)\nprint(dataset[0]['text'][:800] + \"...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configure Training\n",
    "\n",
    "Set up training hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from trl import SFTTrainer\nfrom transformers import TrainingArguments\n\n# Training configuration (Optimized for Tool Calling)\ntraining_args = TrainingArguments(\n    # Output\n    output_dir = \"outputs/network-security-v2\",\n    \n    # Batch size & accumulation\n    per_device_train_batch_size = 2,\n    gradient_accumulation_steps = 4,  # Effective batch size = 8\n    \n    # Training duration\n    num_train_epochs = 3,\n    \n    # Learning rate (Optimized for tool calling)\n    learning_rate = 3e-4,  # Higher LR for tool calling\n    lr_scheduler_type = \"cosine\",  # Cosine decay\n    warmup_ratio = 0.03,  # 3% warmup\n    \n    # Optimization\n    weight_decay = 0.01,\n    optim = \"adamw_8bit\",\n    \n    # Logging & saving\n    logging_steps = 25,\n    save_strategy = \"steps\",\n    save_steps = 500,\n    save_total_limit = 2,\n    \n    # Mixed precision\n    fp16 = not torch.cuda.is_bf16_supported(),\n    bf16 = torch.cuda.is_bf16_supported(),\n    \n    # Misc\n    seed = 42,\n    report_to = \"none\",\n)\n\nprint(\"‚úì Training configuration (Optimized for Tool Calling):\")\nprint(f\"  - Epochs: {training_args.num_train_epochs}\")\nprint(f\"  - Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"  - Learning rate: {training_args.learning_rate}\")\nprint(f\"  - LR Scheduler: {training_args.lr_scheduler_type}\")\nprint(f\"  - Warmup ratio: {training_args.warmup_ratio}\")\nprint(f\"  - Mixed precision: {'BF16' if training_args.bf16 else 'FP16'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Initialize Trainer\n",
    "\n",
    "Create the SFTTrainer for supervised fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,  # Can make training 5x faster for short sequences\n",
    "    args = training_args,\n",
    ")\n",
    "\n",
    "print(\"‚úì Trainer initialized\")\n",
    "print(f\"\\nTraining {len(dataset)} examples\")\n",
    "print(f\"Estimated training time: ~{len(dataset) * 3 / 3600:.1f} hours (very rough estimate)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Start Training\n",
    "\n",
    "Begin the fine-tuning process. Monitor the loss - it should decrease over time.\n",
    "\n",
    "**Target loss:** 0.5-1.0 is generally good\n",
    "**Red flags:**\n",
    "- Loss not decreasing ‚Üí adjust learning rate\n",
    "- Loss near 0 ‚Üí overfitting, reduce epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "print(\"Watch the loss values - they should decrease over time.\\n\")\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Final loss: {trainer_stats.training_loss:.4f}\")\n",
    "print(f\"Training time: {trainer_stats.metrics['train_runtime']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test the Model\n",
    "\n",
    "Try out your fine-tuned model with some network security questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test questions\n",
    "test_questions = [\n",
    "    \"How do I configure port security on a Cisco switch?\",\n",
    "    \"Explain the difference between AWS Security Groups and Network ACLs.\",\n",
    "    \"What Snort rules would detect SQL injection attempts?\",\n",
    "    \"My VPN tunnel keeps dropping. How do I troubleshoot this?\",\n",
    "]\n",
    "\n",
    "print(\"Testing the fine-tuned model...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\nTest {i}/{len(test_questions)}\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"Question: {question}\\n\")\n",
    "    \n",
    "    # Format as chat\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    # Generate response\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    \n",
    "    # Decode and print\n",
    "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    # Extract just the assistant's response\n",
    "    response = response.split(\"assistant\\n\\n\")[-1] if \"assistant\" in response else response\n",
    "    \n",
    "    print(f\"Answer:\\n{response}\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "print(\"\\n‚úÖ Testing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save the Model\n",
    "\n",
    "Save the LoRA adapter (small ~100-200MB file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save locally\n",
    "model.save_pretrained(\"../models/network-security-lora\")\n",
    "tokenizer.save_pretrained(\"../models/network-security-lora\")\n",
    "\n",
    "print(\"‚úì Model saved locally to: models/network-security-lora\")\n",
    "print(\"\\nOptional: Upload to Hugging Face Hub\")\n",
    "print(\"Uncomment and run the cell below to upload.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Push to Hugging Face Hub\n",
    "# Replace 'your-username' with your HF username\n",
    "\n",
    "# model.push_to_hub(\"your-username/llama3-network-security-lora\", token=\"YOUR_HF_TOKEN\")\n",
    "# tokenizer.push_to_hub(\"your-username/llama3-network-security-lora\", token=\"YOUR_HF_TOKEN\")\n",
    "\n",
    "# print(\"‚úì Model uploaded to Hugging Face Hub!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export to GGUF for Ollama\n",
    "\n",
    "Convert to GGUF format for use with Ollama on your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# First, save merged 16-bit model\nprint(\"Step 1: Merging LoRA with base model...\")\nmodel.save_pretrained_merged(\n    \"models/merged-16bit\",\n    tokenizer,\n    save_method=\"merged_16bit\"\n)\nprint(\"‚úì Merged model saved\\n\")\n\n# Convert to GGUF with multiple quantization levels\nprint(\"Step 2: Converting to GGUF format...\")\nprint(\"This will create 3 quantized versions (Q4_K_M, Q5_K_M, Q8_0)\\n\")\n\nmodel.save_pretrained_gguf(\n    \"models/gguf\",\n    tokenizer,\n    quantization_method=[\"q4_k_m\", \"q5_k_m\", \"q8_0\"]\n)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"‚úÖ GGUF CONVERSION COMPLETE!\")\nprint(\"=\"*80)\nprint(\"\\nCreated files:\")\nprint(\"  - models/gguf/unsloth.Q4_K_M.gguf (~4.5GB) - Fastest, good quality\")\nprint(\"  - models/gguf/unsloth.Q5_K_M.gguf (~5.5GB) - Balanced [RECOMMENDED]\")\nprint(\"  - models/gguf/unsloth.Q8_0.gguf (~8GB) - Highest quality\")\nprint(\"\\nNext steps:\")\nprint(\"1. Download the Q4_K_M or Q5_K_M file\")\nprint(\"2. Rename to: network-security-expert-v2.Q4_K_M.gguf\")\nprint(\"3. Place in v2/models/gguf/ folder\")\nprint(\"4. Run: cd v2/models && ollama create network-security-expert-v2 -f Modelfile\")\nprint(\"5. Test: ollama run network-security-expert-v2\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've fine-tuned Llama 3 8B to be a Network Security expert.\n",
    "\n",
    "### What You've Done:\n",
    "1. ‚úÖ Loaded Llama 3.1 8B Instruct with 4-bit quantization\n",
    "2. ‚úÖ Configured LoRA adapters for efficient training\n",
    "3. ‚úÖ Trained on your network security dataset\n",
    "4. ‚úÖ Tested the model with example questions\n",
    "5. ‚úÖ Saved the LoRA adapter\n",
    "6. ‚úÖ Converted to GGUF for Ollama deployment\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "**To use with Ollama:**\n",
    "```bash\n",
    "# 1. Create Modelfile (see models/Modelfile)\n",
    "cd models\n",
    "ollama create network-security-expert -f Modelfile\n",
    "\n",
    "# 2. Run your model\n",
    "ollama run network-security-expert\n",
    "```\n",
    "\n",
    "**To improve your model:**\n",
    "- Generate more training data for weak areas\n",
    "- Increase training epochs if underfitting\n",
    "- Add more diverse scenarios and edge cases\n",
    "- Combine with RAG for up-to-date CVE information\n",
    "\n",
    "**Questions or issues?**\n",
    "- Check the plan file for troubleshooting tips\n",
    "- Review training loss curves\n",
    "- Validate your dataset quality\n",
    "\n",
    "üéâ Happy network securing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}