{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Fine-tuning Llama 3.1 8B for Network Security Expert v2\n\nThis notebook fine-tunes Llama 3.1 8B Instruct to create a specialized **Network Security Expert AI** with:\n- **Advanced tool calling** using native Llama 3.1 format (`<|python_tag|>`)\n- **FireWeave orchestration** capabilities\n- **Infosec conversational expertise**\n\n**Training Configuration (Local VM - RTX 3090/4090 Optimized):**\n- LoRA: r=32, alpha=32, dropout=0, **rsLoRA enabled**\n- Learning rate: 2e-4 with cosine scheduler\n- Max sequence length: 2048 (covers 99% of training data)\n- **Packing disabled** for stability\n- **NEFTune noise** (alpha=5) for better generalization\n- Batch size: 2 with gradient accumulation 4 (effective=8)\n\n**Runtime:** Local Ubuntu VM with GPU passthrough (RTX 3090/4090)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "Run this cell if packages aren't installed yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Environment setup for local VM (packages already installed via pip)\nimport os\n\n# Prevent Triton timeout issues\nos.environ[\"TRITON_CACHE_MANAGER\"] = \"unsloth.triton_cache:TritonCacheManager\"\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\"\n\n# Clear Triton cache if it exists\nimport shutil\nfrom pathlib import Path\ntriton_cache = Path.home() / \".triton\" / \"cache\"\nif triton_cache.exists():\n    print(f\"Clearing stale Triton cache...\")\n    shutil.rmtree(triton_cache, ignore_errors=True)\n\nprint(\"‚úì Environment configured for stable training\")\nprint(\"‚úì Running on local VM (packages pre-installed)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model\n",
    "\n",
    "Load Llama 3.1 8B Instruct with 4-bit quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from unsloth import FastLanguageModel\nimport torch\nimport os\n\n# Fix for Triton timeout issues on some systems\nos.environ[\"TRITON_CACHE_MANAGER\"] = \"unsloth.triton_cache:TritonCacheManager\"\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\"  # Set to \"1\" only for debugging\n\n# Configuration (Optimized for stability)\nmax_seq_length = 2048  # Reduced from 8192 - covers 99% of training data\ndtype = None  # Auto-detect (bfloat16 for Ampere+)\nload_in_4bit = True  # Use 4-bit quantization for QLoRA\n\n# Load model\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/llama-3.1-8b-Instruct-bnb-4bit\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n)\n\nprint(f\"‚úì Model loaded: Llama 3.1 8B Instruct (4-bit)\")\nprint(f\"‚úì Max sequence length: {max_seq_length}\")\nprint(f\"‚úì Data type: {dtype if dtype else 'Auto (bf16 on Ampere+)'}\")\n\n# GPU memory info\nif torch.cuda.is_available():\n    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3\n    gpu_used = torch.cuda.memory_allocated(0) / 1024**3\n    print(f\"‚úì GPU: {torch.cuda.get_device_name(0)} ({gpu_mem:.1f} GB total, {gpu_used:.1f} GB used)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Configure LoRA (2025 Best Practices)\n\nAdd LoRA adapters with **rsLoRA** (rank-stabilized LoRA) for better scaling at higher ranks."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Add LoRA adapters (2025 Best Practices with rsLoRA)\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 32,  # Higher rank with rsLoRA (was 16)\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha = 32,  # Match rank when using rsLoRA\n    lora_dropout = 0,  # Unsloth recommends 0 dropout\n    bias = \"none\",\n    use_gradient_checkpointing = \"unsloth\",  # 30% less VRAM\n    random_state = 3407,\n    use_rslora = True,  # NEW: Rank-stabilized LoRA for better high-rank performance\n    loftq_config = None,\n)\n\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\n\nprint(\"‚úì LoRA adapters configured (2025 Best Practices)\")\nprint(f\"  - Rank: 32 (increased from 16)\")\nprint(f\"  - Alpha: 32 (matched with rank for rsLoRA)\")\nprint(f\"  - Dropout: 0 (Unsloth recommended)\")\nprint(f\"  - rsLoRA: Enabled (scales by 1/sqrt(r) instead of 1/r)\")\nprint(f\"  - Target modules: All attention and MLP layers\")\nprint(f\"  - Trainable parameters: ~{trainable_params / 1e6:.1f}M ({100 * trainable_params / total_params:.2f}%)\")\nprint(f\"\\n[rsLoRA] Better performance at higher ranks with proper gradient scaling\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Dataset\n",
    "\n",
    "Load your network security training data in ChatML/ShareGPT format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from datasets import load_dataset\nimport json\nimport os\n\n# Local dataset path - adjust if needed\ndataset_path = os.path.expanduser(\"~/finetuning/data/processed/all_training_data.json\")\n\n# Alternative paths to try\nalt_paths = [\n    \"data/processed/all_training_data.json\",\n    \"../data/processed/all_training_data.json\",\n    os.path.expanduser(\"~/finetuning/data/processed/high_quality_new.json\"),\n]\n\n# Find the dataset\nif not os.path.exists(dataset_path):\n    for alt in alt_paths:\n        if os.path.exists(alt):\n            dataset_path = alt\n            break\n\ntry:\n    dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n    print(f\"‚úì Dataset loaded from: {dataset_path}\")\n    print(f\"‚úì Total examples: {len(dataset)}\")\n    \n    # Count tool calling vs conversational\n    tool_count = sum(1 for ex in dataset if '<|python_tag|>' in str(ex.get('conversations', [])))\n    print(f\"  - Tool calling: {tool_count} ({100*tool_count/len(dataset):.1f}%)\")\n    print(f\"  - Conversational: {len(dataset) - tool_count}\")\n    \n    # Show a sample\n    print(\"\\nSample conversation:\")\n    print(\"-\" * 80)\n    sample = dataset[0]['conversations']\n    for msg in sample[:2]:\n        role = msg['from']\n        text = msg['value'][:200]\n        print(f\"{role.upper()}: {text}...\\n\")\n    \nexcept FileNotFoundError:\n    print(f\"‚ùå Dataset not found!\")\n    print(f\"\\nSearched locations:\")\n    print(f\"  - {dataset_path}\")\n    for alt in alt_paths:\n        print(f\"  - {alt}\")\n    print(\"\\nMake sure your training data is in ~/finetuning/data/processed/\")\n    raise"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Format Dataset for Training\n",
    "\n",
    "Apply Llama 3 chat template to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from unsloth.chat_templates import get_chat_template\nimport json\n\n# Apply Llama 3.1 chat template\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"llama-3.1\",\n)\n\n# System prompt for tool calling\nSYSTEM_PROMPT = \"\"\"You are a Network Security Expert AI with FireWeave orchestration capabilities.\n\nAvailable tools: check_traffic_flow, analyze_attack_path, run_compliance_scan, find_shadowed_rules, create_firewall_rule, get_rule_hit_count, calculate_blast_radius, fetch_jira_issues\n\nWhen calling tools, use the format: <|python_tag|>{\"name\": \"tool_name\", \"parameters\": {...}}\n\nProvide accurate, detailed technical guidance with specific commands and configurations.\"\"\"\n\ndef formatting_prompts_func(examples):\n    \"\"\"Format conversations for Llama 3.1 native tool calling.\"\"\"\n    conversations = examples[\"conversations\"]\n    tools_list = examples.get(\"tools\", [None] * len(conversations))\n    texts = []\n    \n    for convo, tools in zip(conversations, tools_list):\n        # Build text manually for proper tool calling format\n        text = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n        text += SYSTEM_PROMPT\n        \n        # Add tool definitions if available\n        if tools:\n            text += \"\\n\\nAvailable tools:\\n\"\n            text += json.dumps(tools, indent=2)\n        \n        text += \"<|eot_id|>\"\n        \n        for turn in convo:\n            role = turn.get(\"from\", \"\")\n            value = turn.get(\"value\", \"\")\n            \n            if role == \"human\":\n                text += f\"<|start_header_id|>user<|end_header_id|>\\n\\n{value}<|eot_id|>\"\n            elif role == \"gpt\":\n                # Check if this is a tool call (contains <|python_tag|>)\n                if \"<|python_tag|>\" in value:\n                    # Tool call ends with <|eom_id|> (end of message, expecting tool response)\n                    text += f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n{value}<|eom_id|>\"\n                else:\n                    # Regular response ends with <|eot_id|>\n                    text += f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n{value}<|eot_id|>\"\n            elif role == \"tool\":\n                # Tool response uses ipython role\n                text += f\"<|start_header_id|>ipython<|end_header_id|>\\n\\n{value}<|eot_id|>\"\n        \n        texts.append(text)\n    \n    return {\"text\": texts}\n\n# Apply formatting\ndataset = dataset.map(formatting_prompts_func, batched=True)\n\nprint(\"‚úì Native Llama 3.1 tool calling format applied\")\nprint(\"\\nFormatted example (first 800 chars):\")\nprint(\"-\" * 80)\nprint(dataset[0]['text'][:800] + \"...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Configure Training (Stability Optimized)\n\nSet up training hyperparameters optimized for stability:\n- **Batch size 1**: Prevents Triton kernel timeout\n- **Gradient accumulation 8**: Maintains effective batch size of 8\n- **No packing**: Avoids creating very long packed sequences"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from trl import SFTTrainer\nfrom transformers import TrainingArguments\nimport os\n\n# Create output directory\nos.makedirs(\"outputs/network-security-v2\", exist_ok=True)\n\n# Training configuration (Optimized for RTX 3090/4090 with 24GB VRAM)\ntraining_args = TrainingArguments(\n    # Output\n    output_dir = \"outputs/network-security-v2\",\n    \n    # Batch size - increased for 24GB VRAM\n    per_device_train_batch_size = 2,  # Can use 2 with 24GB VRAM\n    gradient_accumulation_steps = 4,  # Effective batch size = 8\n    \n    # Training duration\n    num_train_epochs = 3,\n    \n    # Learning rate (standard for QLoRA)\n    learning_rate = 2e-4,\n    lr_scheduler_type = \"cosine\",\n    warmup_ratio = 0.03,\n    \n    # Optimization\n    weight_decay = 0.01,\n    max_grad_norm = 0.3,  # Gradient clipping for stability\n    optim = \"adamw_8bit\",\n    \n    # Logging & saving\n    logging_steps = 10,\n    save_strategy = \"steps\",\n    save_steps = 500,\n    save_total_limit = 2,\n    \n    # Mixed precision\n    fp16 = not torch.cuda.is_bf16_supported(),\n    bf16 = torch.cuda.is_bf16_supported(),\n    \n    # Misc\n    seed = 3407,\n    report_to = \"none\",\n    \n    # Local VM settings\n    dataloader_num_workers = 2,  # Can use more workers locally\n)\n\nprint(\"‚úì Training configuration (RTX 3090/4090 Optimized):\")\nprint(f\"  - Epochs: {training_args.num_train_epochs}\")\nprint(f\"  - Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"  - Gradient accumulation: {training_args.gradient_accumulation_steps}\")\nprint(f\"  - Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"  - Learning rate: {training_args.learning_rate}\")\nprint(f\"  - LR Scheduler: {training_args.lr_scheduler_type}\")\nprint(f\"  - Warmup ratio: {training_args.warmup_ratio}\")\nprint(f\"  - Gradient clipping: {training_args.max_grad_norm}\")\nprint(f\"  - Mixed precision: {'BF16' if training_args.bf16 else 'FP16'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Initialize Trainer (Stability Settings)\n\nCreate the SFTTrainer with stability optimizations:\n- **Packing disabled**: Prevents creating extremely long sequences that timeout\n- **NEFTune**: Noisy embeddings for better generalization\n- **Single process**: Avoids multiprocessing issues in local runtime"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from unsloth import is_bfloat16_supported\n\n# Stability settings (packing disabled to prevent Triton timeout)\nUSE_PACKING = False     # Disabled - can cause very long sequences and timeout\nNEFTUNE_ALPHA = 5.0     # Noisy embeddings for generalization\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 1,  # Single process for stability\n    packing = USE_PACKING,\n    neftune_noise_alpha = NEFTUNE_ALPHA,\n    args = training_args,\n)\n\nprint(\"‚úì Trainer initialized (Stability Optimized)\")\nprint(f\"\\n[Configuration]\")\nprint(f\"  Packing: {USE_PACKING} (disabled for stability)\")\nprint(f\"  NEFTune: alpha={NEFTUNE_ALPHA}\")\nprint(f\"  Max seq length: {max_seq_length}\")\nprint(f\"\\nTraining {len(dataset)} examples\")\nprint(f\"Estimated steps per epoch: ~{len(dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)}\")\nprint(f\"\\nNote: If training still times out, try reducing max_seq_length to 1024\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Start Training\n\nBegin the fine-tuning process. This cell first clears the Triton cache to prevent stale kernel issues.\n\n**Target loss:** 0.5-1.0 is generally good\n**Red flags:**\n- Loss not decreasing ‚Üí adjust learning rate\n- Loss near 0 ‚Üí overfitting, reduce epochs\n- \"Triton Error: launch timed out\" ‚Üí reduce batch size or sequence length\n\n**Common warnings (can be ignored):**\n- \"Model is already on multiple devices\" - normal for QLoRA\n- NEFTune warnings - normal during training"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clear any stale Triton cache before training\nimport shutil\nfrom pathlib import Path\n\ntriton_cache = Path.home() / \".triton\" / \"cache\"\nif triton_cache.exists():\n    print(f\"Clearing Triton cache at {triton_cache}...\")\n    shutil.rmtree(triton_cache, ignore_errors=True)\n    print(\"‚úì Triton cache cleared\")\n\n# Note about \"model is already on multiple devices\" warning\nprint(\"\\nNote: 'Model is already on multiple devices' warning is normal and can be ignored.\")\nprint(\"=\"*50)\n\n# Start training\nprint(\"\\nStarting v2 training...\")\nprint(f\"Training on {len(dataset)} examples\")\nprint(f\"This will take several hours depending on your GPU.\")\nprint(\"=\"*50)\n\ntrainer_stats = trainer.train()\n\nprint(\"=\"*50)\nprint(\"Training complete!\")\nprint(f\"Final loss: {trainer_stats.training_loss:.4f}\")\nprint(f\"Training time: {trainer_stats.metrics['train_runtime']/3600:.2f} hours\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test the Model\n",
    "\n",
    "Try out your fine-tuned model with some network security questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test questions\n",
    "test_questions = [\n",
    "    \"How do I configure port security on a Cisco switch?\",\n",
    "    \"Explain the difference between AWS Security Groups and Network ACLs.\",\n",
    "    \"What Snort rules would detect SQL injection attempts?\",\n",
    "    \"My VPN tunnel keeps dropping. How do I troubleshoot this?\",\n",
    "]\n",
    "\n",
    "print(\"Testing the fine-tuned model...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\nTest {i}/{len(test_questions)}\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"Question: {question}\\n\")\n",
    "    \n",
    "    # Format as chat\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    # Generate response\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    \n",
    "    # Decode and print\n",
    "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    # Extract just the assistant's response\n",
    "    response = response.split(\"assistant\\n\\n\")[-1] if \"assistant\" in response else response\n",
    "    \n",
    "    print(f\"Answer:\\n{response}\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "print(\"\\n‚úÖ Testing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save the Model\n",
    "\n",
    "Save the LoRA adapter (small ~100-200MB file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\n# Create models directory\nos.makedirs(\"models/network-security-lora\", exist_ok=True)\n\n# Save LoRA adapter locally\nmodel.save_pretrained(\"models/network-security-lora\")\ntokenizer.save_pretrained(\"models/network-security-lora\")\n\nprint(\"‚úì LoRA adapter saved to: models/network-security-lora\")\nprint(f\"  Size: ~100-200 MB\")\nprint(\"\\nTo upload to Hugging Face Hub, uncomment the cell below.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Push to Hugging Face Hub\n",
    "# Replace 'your-username' with your HF username\n",
    "\n",
    "# model.push_to_hub(\"your-username/llama3-network-security-lora\", token=\"YOUR_HF_TOKEN\")\n",
    "# tokenizer.push_to_hub(\"your-username/llama3-network-security-lora\", token=\"YOUR_HF_TOKEN\")\n",
    "\n",
    "# print(\"‚úì Model uploaded to Hugging Face Hub!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export to GGUF for Ollama\n",
    "\n",
    "Convert to GGUF format for use with Ollama on your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\n# Create output directories\nos.makedirs(\"models/merged-16bit\", exist_ok=True)\nos.makedirs(\"models/gguf\", exist_ok=True)\n\n# First, save merged 16-bit model\nprint(\"Step 1: Merging LoRA with base model...\")\nprint(\"(This may take a few minutes...)\")\nmodel.save_pretrained_merged(\n    \"models/merged-16bit\",\n    tokenizer,\n    save_method=\"merged_16bit\"\n)\nprint(\"‚úì Merged model saved\\n\")\n\n# Convert to GGUF with multiple quantization levels\nprint(\"Step 2: Converting to GGUF format...\")\nprint(\"This will create 3 quantized versions (Q4_K_M, Q5_K_M, Q8_0)\")\nprint(\"(This takes 10-30 minutes depending on your CPU...)\\n\")\n\nmodel.save_pretrained_gguf(\n    \"models/gguf\",\n    tokenizer,\n    quantization_method=[\"q4_k_m\", \"q5_k_m\", \"q8_0\"]\n)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"‚úÖ GGUF CONVERSION COMPLETE!\")\nprint(\"=\"*80)\nprint(\"\\nCreated files in models/gguf/:\")\nprint(\"  - unsloth.Q4_K_M.gguf (~4.5GB) - Fastest, good quality\")\nprint(\"  - unsloth.Q5_K_M.gguf (~5.5GB) - Balanced [RECOMMENDED]\")\nprint(\"  - unsloth.Q8_0.gguf (~8GB) - Highest quality\")\nprint(\"\\n\" + \"=\"*80)\nprint(\"NEXT STEPS (run in terminal):\")\nprint(\"=\"*80)\nprint(\"\"\"\n# 1. Rename the GGUF file\nmv models/gguf/unsloth.Q4_K_M.gguf models/gguf/network-security-expert.Q4_K_M.gguf\n\n# 2. Create Ollama model\ncd models\nollama create network-security-expert -f Modelfile\n\n# 3. Test your model\nollama run network-security-expert\n\"\"\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nYou've fine-tuned Llama 3.1 8B to be a Network Security expert on your local VM!\n\n### What You've Done:\n1. ‚úÖ Loaded Llama 3.1 8B Instruct with 4-bit quantization\n2. ‚úÖ Configured LoRA adapters for efficient training\n3. ‚úÖ Trained on your network security dataset\n4. ‚úÖ Tested the model with example questions\n5. ‚úÖ Saved the LoRA adapter\n6. ‚úÖ Converted to GGUF for Ollama deployment\n\n### Files Created:\n- `models/network-security-lora/` - LoRA adapter (~100-200MB)\n- `models/merged-16bit/` - Full merged model\n- `models/gguf/*.gguf` - Quantized models for Ollama\n\n### Deploy with Ollama:\n```bash\n# Install Ollama (if not already)\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Create and run your model\ncd ~/finetuning/models\nollama create network-security-expert -f Modelfile\nollama run network-security-expert\n```\n\n### Test Your Model:\n```\n>>> How do I configure a Palo Alto firewall rule?\n>>> What's the difference between IDS and IPS?\n>>> Explain zero trust architecture\n```\n\nüéâ Training complete!"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}