{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Llama 3 8B for Network Security Expert\n",
    "\n",
    "This notebook fine-tunes Llama 3 8B Instruct to become a specialized Network Security expert using Unsloth with QLoRA (4-bit quantization).\n",
    "\n",
    "**Topics covered:**\n",
    "- Firewall & Network Device Configuration (Cisco, Palo Alto, Fortinet)\n",
    "- Cloud Security (AWS, Azure, GCP)\n",
    "- Threat Detection & Incident Response (IDS/IPS, SIEM)\n",
    "\n",
    "**Training Method:** QLoRA (4-bit quantization) for efficient training on consumer GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "Run this cell if packages aren't installed yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Unsloth and dependencies\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model\n",
    "\n",
    "Load Llama 3.1 8B Instruct with 4-bit quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Configuration\n",
    "max_seq_length = 2048  # Can increase to 4096 if needed\n",
    "dtype = None  # Auto-detect (float16 for T4, bfloat16 for Ampere+)\n",
    "load_in_4bit = True  # Use 4-bit quantization for QLoRA\n",
    "\n",
    "# Load model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/llama-3.1-8b-Instruct-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Model loaded: Llama 3.1 8B Instruct (4-bit)\")\n",
    "print(f\"âœ“ Max sequence length: {max_seq_length}\")\n",
    "print(f\"âœ“ Data type: {dtype if dtype else 'Auto'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure LoRA\n",
    "\n",
    "Add LoRA adapters for parameter-efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,  # LoRA rank\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 16,  # LoRA scaling\n",
    "    lora_dropout = 0,  # 0 for QLoRA\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",  # Memory efficient\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")\n",
    "\n",
    "print(\"âœ“ LoRA adapters configured\")\n",
    "print(f\"  - Rank: 16\")\n",
    "print(f\"  - Target modules: All attention and MLP layers\")\n",
    "print(f\"  - Trainable parameters: ~{sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Dataset\n",
    "\n",
    "Load your network security training data in ChatML/ShareGPT format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load your dataset\n",
    "# Replace with your actual dataset path\n",
    "dataset_path = \"../data/processed/network_security_qa.json\"\n",
    "\n",
    "try:\n",
    "    dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
    "    print(f\"âœ“ Dataset loaded: {len(dataset)} examples\")\n",
    "    \n",
    "    # Show a sample\n",
    "    print(\"\\nSample conversation:\")\n",
    "    print(\"-\" * 80)\n",
    "    sample = dataset[0]['conversations']\n",
    "    for msg in sample:\n",
    "        role = msg['from']\n",
    "        text = msg['value'][:200]\n",
    "        print(f\"{role.upper()}: {text}...\\n\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ Dataset not found at {dataset_path}\")\n",
    "    print(\"\\nPlease create your dataset first:\")\n",
    "    print(\"1. Run: python scripts/generate_synthetic_data.py --provider openai --topic cisco-firewall --count 50\")\n",
    "    print(\"2. Merge topic datasets into data/processed/network_security_qa.json\")\n",
    "    print(\"3. Validate: python scripts/validate_dataset.py data/processed/network_security_qa.json\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Format Dataset for Training\n",
    "\n",
    "Apply Llama 3 chat template to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# Apply Llama 3 chat template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"Format conversations using chat template\"\"\"\n",
    "    conversations = examples[\"conversations\"]\n",
    "    texts = []\n",
    "    for convo in conversations:\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            convo, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Apply formatting\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "print(\"âœ“ Chat template applied\")\n",
    "print(\"\\nFormatted example (first 500 chars):\")\n",
    "print(\"-\" * 80)\n",
    "print(dataset[0]['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configure Training\n",
    "\n",
    "Set up training hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Training configuration\n",
    "training_args = TrainingArguments(\n",
    "    # Output\n",
    "    output_dir = \"../outputs/network-security-lora\",\n",
    "    \n",
    "    # Batch size & accumulation\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 4,  # Effective batch size = 8\n",
    "    \n",
    "    # Training duration\n",
    "    num_train_epochs = 3,  # 1-3 epochs recommended\n",
    "    # max_steps = 60,  # Use this for quick testing instead of epochs\n",
    "    \n",
    "    # Learning rate\n",
    "    learning_rate = 2e-4,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    warmup_steps = 5,\n",
    "    \n",
    "    # Optimization\n",
    "    weight_decay = 0.01,\n",
    "    optim = \"adamw_8bit\",\n",
    "    \n",
    "    # Logging & saving\n",
    "    logging_steps = 10,\n",
    "    save_strategy = \"steps\",\n",
    "    save_steps = 100,\n",
    "    save_total_limit = 2,\n",
    "    \n",
    "    # Mixed precision\n",
    "    fp16 = not torch.cuda.is_bf16_supported(),\n",
    "    bf16 = torch.cuda.is_bf16_supported(),\n",
    "    \n",
    "    # Misc\n",
    "    seed = 3407,\n",
    "    report_to = \"none\",  # Disable wandb/tensorboard\n",
    ")\n",
    "\n",
    "print(\"âœ“ Training configuration:\")\n",
    "print(f\"  - Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  - Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  - Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  - Mixed precision: {'BF16' if training_args.bf16 else 'FP16'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Initialize Trainer\n",
    "\n",
    "Create the SFTTrainer for supervised fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,  # Can make training 5x faster for short sequences\n",
    "    args = training_args,\n",
    ")\n",
    "\n",
    "print(\"âœ“ Trainer initialized\")\n",
    "print(f\"\\nTraining {len(dataset)} examples\")\n",
    "print(f\"Estimated training time: ~{len(dataset) * 3 / 3600:.1f} hours (very rough estimate)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Start Training\n",
    "\n",
    "Begin the fine-tuning process. Monitor the loss - it should decrease over time.\n",
    "\n",
    "**Target loss:** 0.5-1.0 is generally good\n",
    "**Red flags:**\n",
    "- Loss not decreasing â†’ adjust learning rate\n",
    "- Loss near 0 â†’ overfitting, reduce epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "print(\"Watch the loss values - they should decrease over time.\\n\")\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Final loss: {trainer_stats.training_loss:.4f}\")\n",
    "print(f\"Training time: {trainer_stats.metrics['train_runtime']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test the Model\n",
    "\n",
    "Try out your fine-tuned model with some network security questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test questions\n",
    "test_questions = [\n",
    "    \"How do I configure port security on a Cisco switch?\",\n",
    "    \"Explain the difference between AWS Security Groups and Network ACLs.\",\n",
    "    \"What Snort rules would detect SQL injection attempts?\",\n",
    "    \"My VPN tunnel keeps dropping. How do I troubleshoot this?\",\n",
    "]\n",
    "\n",
    "print(\"Testing the fine-tuned model...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\nTest {i}/{len(test_questions)}\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"Question: {question}\\n\")\n",
    "    \n",
    "    # Format as chat\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    # Generate response\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    \n",
    "    # Decode and print\n",
    "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    # Extract just the assistant's response\n",
    "    response = response.split(\"assistant\\n\\n\")[-1] if \"assistant\" in response else response\n",
    "    \n",
    "    print(f\"Answer:\\n{response}\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "print(\"\\nâœ… Testing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save the Model\n",
    "\n",
    "Save the LoRA adapter (small ~100-200MB file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save locally\n",
    "model.save_pretrained(\"../models/network-security-lora\")\n",
    "tokenizer.save_pretrained(\"../models/network-security-lora\")\n",
    "\n",
    "print(\"âœ“ Model saved locally to: models/network-security-lora\")\n",
    "print(\"\\nOptional: Upload to Hugging Face Hub\")\n",
    "print(\"Uncomment and run the cell below to upload.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Push to Hugging Face Hub\n",
    "# Replace 'your-username' with your HF username\n",
    "\n",
    "# model.push_to_hub(\"your-username/llama3-network-security-lora\", token=\"YOUR_HF_TOKEN\")\n",
    "# tokenizer.push_to_hub(\"your-username/llama3-network-security-lora\", token=\"YOUR_HF_TOKEN\")\n",
    "\n",
    "# print(\"âœ“ Model uploaded to Hugging Face Hub!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export to GGUF for Ollama\n",
    "\n",
    "Convert to GGUF format for use with Ollama on your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, save merged 16-bit model\n",
    "print(\"Step 1: Merging LoRA with base model...\")\n",
    "model.save_pretrained_merged(\n",
    "    \"../models/merged-16bit\",\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\"\n",
    ")\n",
    "print(\"âœ“ Merged model saved\\n\")\n",
    "\n",
    "# Convert to GGUF with multiple quantization levels\n",
    "print(\"Step 2: Converting to GGUF format...\")\n",
    "print(\"This will create 3 quantized versions (Q4_K_M, Q5_K_M, Q8_0)\\n\")\n",
    "\n",
    "model.save_pretrained_gguf(\n",
    "    \"../models/gguf\",\n",
    "    tokenizer,\n",
    "    quantization_method=[\"q4_k_m\", \"q5_k_m\", \"q8_0\"]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… GGUF CONVERSION COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nCreated files:\")\n",
    "print(\"  - models/gguf/unsloth.Q4_K_M.gguf (~4.5GB) - Fastest, good quality\")\n",
    "print(\"  - models/gguf/unsloth.Q5_K_M.gguf (~5.5GB) - Balanced [RECOMMENDED]\")\n",
    "print(\"  - models/gguf/unsloth.Q8_0.gguf (~8GB) - Highest quality\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Create Modelfile in models/ directory\")\n",
    "print(\"2. Run: ollama create network-security-expert -f models/Modelfile\")\n",
    "print(\"3. Test: ollama run network-security-expert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've fine-tuned Llama 3 8B to be a Network Security expert.\n",
    "\n",
    "### What You've Done:\n",
    "1. âœ… Loaded Llama 3.1 8B Instruct with 4-bit quantization\n",
    "2. âœ… Configured LoRA adapters for efficient training\n",
    "3. âœ… Trained on your network security dataset\n",
    "4. âœ… Tested the model with example questions\n",
    "5. âœ… Saved the LoRA adapter\n",
    "6. âœ… Converted to GGUF for Ollama deployment\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "**To use with Ollama:**\n",
    "```bash\n",
    "# 1. Create Modelfile (see models/Modelfile)\n",
    "cd models\n",
    "ollama create network-security-expert -f Modelfile\n",
    "\n",
    "# 2. Run your model\n",
    "ollama run network-security-expert\n",
    "```\n",
    "\n",
    "**To improve your model:**\n",
    "- Generate more training data for weak areas\n",
    "- Increase training epochs if underfitting\n",
    "- Add more diverse scenarios and edge cases\n",
    "- Combine with RAG for up-to-date CVE information\n",
    "\n",
    "**Questions or issues?**\n",
    "- Check the plan file for troubleshooting tips\n",
    "- Review training loss curves\n",
    "- Validate your dataset quality\n",
    "\n",
    "ðŸŽ‰ Happy network securing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}